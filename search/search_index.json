{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome This is a place for community-driven Canvas shader development guides.","title":"Welcome"},{"location":"#welcome","text":"This is a place for community-driven Canvas shader development guides.","title":"Welcome"},{"location":"pipeline/colored_lights/","text":"Colored Lights (WIP) This is a feature in Canvas that adds client-side colored lighting using the flood-fill algorithm. The data is firstly and priorly absorbed from the block light API, then the item light API, and last fallback being the vanilla light level. The algorithm works with light propagation hooking to a region (chunk) building for block updates, which was already optimized through occlusion culling. This ensures minimal memory usage. The actual propagation and texture uploads are done in the Render Thread. Only queued regions are updated to minimize impact on frame time. Block Light API This is expressed with json files that contain color (rgb) data and light level data (intensity) . With two root objects, you can specify the default light values (for the normal default block state) using the \"defaultLight\" object, and the variants (for each unique block state) light values using the \"variants\" object. Inside those objects, you can express the RGB color data with \"red\": , \"green\": and \"blue\":\" . The intensity is expressed through lightLevel\": within the 0-15 range. The fields are optional and may fallback to the fields in \"defaultLight\" if not present, and if there is no \"defaultlight\" object, it will simply fall back to the values that are server-side . These files must be contained in the following paths accordingly: namespace:lights/block , namespace:lights/fluid . The API is designed based on both Material Map and Item Light APIs . Example json: { \"defaultLight\": { \"red\": 0.2, \"green\": 0.2, \"blue\": 0.2 }, \"variants\": { \"color=red\": { \"lightLevel\": 14.0, \"red\": 1.0 }, \"color=green\": { \"lightLevel\": 14.0, \"green\": 1.0 }, \"color=blue\": { \"lightLevel\": 14.0, \"blue\": 1.0 } } } Note that due to this feature being client-side, it won't affect server-side light values. To achieve harmony between visuals and gameplay, mods would require synchronizing server-side light values via other means. Care needs to be taken as luminance is perceived differently in colored lights than vanilla lighting. Pipeline API This feature is not recognized by default. To ensure your pipeline has the colored lighting feature on, you must include it in the json file of the pipeline as an object like so: coloredLights: { useOcclusionData: false } Shader API To take advantage of this feature in your shader files, you can call it using specific includes (depending on what functions you want, and what type of shaders you are using the feature on). Material & Pipeline Write Shaders #ifdef COLORED_LIGHTS_ENABLED uniform sampler2D frxs_lightData; vec4 frx_getLightFiltered(worldPos); vec4 frx_getLightRaw(worldPos); vec3 frx_getLight(worldPos, fallback); #endif You get a predefined sampler of the light data, with functions that are explained below. Pipeline Pass Shaders Including the path frex:shaders/api/light.glsl to your shader program, you gain access to multiple functions like: vec4 frx_getLightFiltered(sampler2D lightSampler, vec3 worldPos); vec4 frx_getLightRaw(sampler2D lightSampler, vec3 worldPos); vec3 frx_getLight(sampler2D lightSampler, vec3 worldPos, vec3 fallback); bool frx_lightDataExists(sampler2D lightSampler, vec3 worldPos); frx_getLightFiltered Function returns the light color with filtering on. frx_getLightRaw Function returns the light color with no filtering, meaning the raw color (per voxel). frx_getLight Function returns the light color with filtering, and a fallback where there is no light data. (Likely, you can use the vanilla lightmap for fallback ( frx_fragLight.x ) frx_lightDataExists Function checks whether or not the light data is uploaded for a specific region in the world Occlusion Data This feature contains occlusion data, and you can define whether or not u want it on in the pipeline json object like so: coloredLights: { useOcclusionData: true } Occlusion data lets you gain access to a function of a variety of data about the light occlusion. struct frx_LightData { vec4 light; bool isLightSource; bool isOccluder; bool isFullCube; }; frx_LightData frx_getLightOcclusionData(sampler2D lightSampler, vec3 worldPos); With a struct, you get the: light unfiltered (raw) light color. isLightSource Determinator for whether a given pixel is a pixel of a light source or not. isOccluder Determinator for whether a given pixel is a pixel of an occluder or not. isFullCube And a determinator for whether a given pixel is a pixel of a full cube or not (regular block unlike a slab or a carpet). frx_getLightOcclusionData Function can give you all that data with a given light sampler, in this case, you may pass \"frex:textures/auto/colored_lights\" to a sampler2D to access the desired data. Normal Offset When sampling the light data, you may run into issues that have to do with certain faces of blocks being dark, or occlusion data not behaving properly. When using the general light functions, you may need to offset the worldPos by the world normals by a slight amount, worldPos + frx_vertexNormal.xyz * 0.05 works out. However, for occlusion data, you may want to watch out for the possibility of shifting the worldPos to a slight negative offset instead of a positive normal offset. Say the worldPos has no offset applied, you may run into cases where you benefit adding a negative offset like so: worldPos - frx_vertexNormal.xyz * 0.01 .","title":"Colored Lights (WIP)"},{"location":"pipeline/colored_lights/#colored-lights-wip","text":"This is a feature in Canvas that adds client-side colored lighting using the flood-fill algorithm. The data is firstly and priorly absorbed from the block light API, then the item light API, and last fallback being the vanilla light level. The algorithm works with light propagation hooking to a region (chunk) building for block updates, which was already optimized through occlusion culling. This ensures minimal memory usage. The actual propagation and texture uploads are done in the Render Thread. Only queued regions are updated to minimize impact on frame time.","title":"Colored Lights (WIP)"},{"location":"pipeline/colored_lights/#block-light-api","text":"This is expressed with json files that contain color (rgb) data and light level data (intensity) . With two root objects, you can specify the default light values (for the normal default block state) using the \"defaultLight\" object, and the variants (for each unique block state) light values using the \"variants\" object. Inside those objects, you can express the RGB color data with \"red\": , \"green\": and \"blue\":\" . The intensity is expressed through lightLevel\": within the 0-15 range. The fields are optional and may fallback to the fields in \"defaultLight\" if not present, and if there is no \"defaultlight\" object, it will simply fall back to the values that are server-side . These files must be contained in the following paths accordingly: namespace:lights/block , namespace:lights/fluid . The API is designed based on both Material Map and Item Light APIs . Example json: { \"defaultLight\": { \"red\": 0.2, \"green\": 0.2, \"blue\": 0.2 }, \"variants\": { \"color=red\": { \"lightLevel\": 14.0, \"red\": 1.0 }, \"color=green\": { \"lightLevel\": 14.0, \"green\": 1.0 }, \"color=blue\": { \"lightLevel\": 14.0, \"blue\": 1.0 } } } Note that due to this feature being client-side, it won't affect server-side light values. To achieve harmony between visuals and gameplay, mods would require synchronizing server-side light values via other means. Care needs to be taken as luminance is perceived differently in colored lights than vanilla lighting.","title":"Block Light API"},{"location":"pipeline/colored_lights/#pipeline-api","text":"This feature is not recognized by default. To ensure your pipeline has the colored lighting feature on, you must include it in the json file of the pipeline as an object like so: coloredLights: { useOcclusionData: false }","title":"Pipeline API"},{"location":"pipeline/colored_lights/#shader-api","text":"To take advantage of this feature in your shader files, you can call it using specific includes (depending on what functions you want, and what type of shaders you are using the feature on).","title":"Shader API"},{"location":"pipeline/colored_lights/#material-pipeline-write-shaders","text":"#ifdef COLORED_LIGHTS_ENABLED uniform sampler2D frxs_lightData; vec4 frx_getLightFiltered(worldPos); vec4 frx_getLightRaw(worldPos); vec3 frx_getLight(worldPos, fallback); #endif You get a predefined sampler of the light data, with functions that are explained below.","title":"Material &amp; Pipeline Write Shaders"},{"location":"pipeline/colored_lights/#pipeline-pass-shaders","text":"Including the path frex:shaders/api/light.glsl to your shader program, you gain access to multiple functions like: vec4 frx_getLightFiltered(sampler2D lightSampler, vec3 worldPos); vec4 frx_getLightRaw(sampler2D lightSampler, vec3 worldPos); vec3 frx_getLight(sampler2D lightSampler, vec3 worldPos, vec3 fallback); bool frx_lightDataExists(sampler2D lightSampler, vec3 worldPos); frx_getLightFiltered Function returns the light color with filtering on. frx_getLightRaw Function returns the light color with no filtering, meaning the raw color (per voxel). frx_getLight Function returns the light color with filtering, and a fallback where there is no light data. (Likely, you can use the vanilla lightmap for fallback ( frx_fragLight.x ) frx_lightDataExists Function checks whether or not the light data is uploaded for a specific region in the world","title":"Pipeline Pass Shaders"},{"location":"pipeline/colored_lights/#occlusion-data","text":"This feature contains occlusion data, and you can define whether or not u want it on in the pipeline json object like so: coloredLights: { useOcclusionData: true } Occlusion data lets you gain access to a function of a variety of data about the light occlusion. struct frx_LightData { vec4 light; bool isLightSource; bool isOccluder; bool isFullCube; }; frx_LightData frx_getLightOcclusionData(sampler2D lightSampler, vec3 worldPos); With a struct, you get the: light unfiltered (raw) light color. isLightSource Determinator for whether a given pixel is a pixel of a light source or not. isOccluder Determinator for whether a given pixel is a pixel of an occluder or not. isFullCube And a determinator for whether a given pixel is a pixel of a full cube or not (regular block unlike a slab or a carpet). frx_getLightOcclusionData Function can give you all that data with a given light sampler, in this case, you may pass \"frex:textures/auto/colored_lights\" to a sampler2D to access the desired data.","title":"Occlusion Data"},{"location":"pipeline/colored_lights/#normal-offset","text":"When sampling the light data, you may run into issues that have to do with certain faces of blocks being dark, or occlusion data not behaving properly. When using the general light functions, you may need to offset the worldPos by the world normals by a slight amount, worldPos + frx_vertexNormal.xyz * 0.05 works out. However, for occlusion data, you may want to watch out for the possibility of shifting the worldPos to a slight negative offset instead of a positive normal offset. Say the worldPos has no offset applied, you may run into cases where you benefit adding a negative offset like so: worldPos - frx_vertexNormal.xyz * 0.01 .","title":"Normal Offset"},{"location":"pipeline/fabulous_pipeline/","text":"Fabulous Pipeline Assign buffers, generally you want to assign the main buffer to the \"solid\".","title":"Fabulous Pipeline"},{"location":"pipeline/fabulous_pipeline/#fabulous-pipeline","text":"Assign buffers, generally you want to assign the main buffer to the \"solid\".","title":"Fabulous Pipeline"},{"location":"pipeline/making_a_pipeline/","text":"Making a Pipeline To get started, let's create a basic Pipeline. Start by creating an empty resource pack. This will be the blank canvas for our Pipeline. Note: everything prefixed with \"my_\" implies any custom name will work. .minecraft/resourcepacks/my_pack/ Create the assets folder and a namespace folder inside it: .minecraft/resourcepacks/my_pack/assets/ .minecraft/resourcepacks/my_pack/assets/my_namespace/ Create a pipelines folder inside the namespace folder: .minecraft/resourcepacks/my_pack/assets/my_namespace/pipelines/ Then inside the pipelines folder, create your Pipeline json5 file: .minecraft/resourcepacks/my_pack/assets/my_namespace/pipelines/my_pipeline.json5 Then, before creating the Pipeline file, generally you want to decide whether to enable Fabulous mode or not. Mode Description without Fabulous mode Faster; All render targets the solid buffer; No composite pass. with Fabulous mode Slightly slower; Separate buffer for each target; Fabulous composite pass. Basic Pipeline Since Canvas ships with a non-fabulous Pipeline called \"Canvas Basic\", we'll call non-fabulous Pipelines \"basic\" Pipelines. To create a basic pipeline, insert the following into your Pipeline json5 file: Fabulous Pipeline Language files","title":"Making a Pipeline"},{"location":"pipeline/making_a_pipeline/#making-a-pipeline","text":"To get started, let's create a basic Pipeline. Start by creating an empty resource pack. This will be the blank canvas for our Pipeline. Note: everything prefixed with \"my_\" implies any custom name will work. .minecraft/resourcepacks/my_pack/ Create the assets folder and a namespace folder inside it: .minecraft/resourcepacks/my_pack/assets/ .minecraft/resourcepacks/my_pack/assets/my_namespace/ Create a pipelines folder inside the namespace folder: .minecraft/resourcepacks/my_pack/assets/my_namespace/pipelines/ Then inside the pipelines folder, create your Pipeline json5 file: .minecraft/resourcepacks/my_pack/assets/my_namespace/pipelines/my_pipeline.json5 Then, before creating the Pipeline file, generally you want to decide whether to enable Fabulous mode or not. Mode Description without Fabulous mode Faster; All render targets the solid buffer; No composite pass. with Fabulous mode Slightly slower; Separate buffer for each target; Fabulous composite pass.","title":"Making a Pipeline"},{"location":"pipeline/making_a_pipeline/#basic-pipeline","text":"Since Canvas ships with a non-fabulous Pipeline called \"Canvas Basic\", we'll call non-fabulous Pipelines \"basic\" Pipelines. To create a basic pipeline, insert the following into your Pipeline json5 file:","title":"Basic Pipeline"},{"location":"pipeline/making_a_pipeline/#fabulous-pipeline","text":"","title":"Fabulous Pipeline"},{"location":"pipeline/making_a_pipeline/#language-files","text":"","title":"Language files"},{"location":"pipeline/material_program/","text":"","title":"Material program"},{"location":"pipeline/shadow_map/","text":"The Shadow Map The shadow map is the image that stores the depth from the sun's perspective. By comparing the depth from the sun's point of view with the depth from the player's point of view, we can tell whether there are shadows. Since this requires rendering the entire scene a second time, it's significantly slower than not using shadows at all. The boring JSON side To start, you need to create the shadow configuration JSON file. You can create this directly in your pipeline description file, or you can use another file and include the file in your pipeline description file. Let's assume you're doing the latter, and create my_shadow_config.json5 . This can be done anywhere under the assets folder. See comments for specific parameter descriptions. { images: [ { name: \"my_shadow_map\", // Size can be whatever you want, but it's recommended to use a power of 2, like 1024 size: 1024, internalFormat: \"DEPTH_COMPONENT32\", pixelFormat: \"DEPTH_COMPONENT\", pixelDataType: \"FLOAT\", target: \"TEXTURE_2D_ARRAY\", depth: 4, texParams: [ {name: \"TEXTURE_MIN_FILTER\", val: \"LINEAR\"}, {name: \"TEXTURE_MAG_FILTER\", val: \"LINEAR\"}, {name: \"TEXTURE_WRAP_S\", val: \"CLAMP_TO_EDGE\"}, {name: \"TEXTURE_WRAP_T\", val: \"CLAMP_TO_EDGE\"}, {name: \"TEXTURE_COMPARE_MODE\", val: \"COMPARE_REF_TO_TEXTURE\"}, {name: \"TEXTURE_COMPARE_FUNC\", val: \"LEQUAL\"} ] } ], framebuffers: [ { name: \"my_shadow_framebuffer\", // The shadow pass only writes to depth, so we provide a depth attachment depthAttachment: { image: \"my_shadow_map\", clearDepth: 1.0 }, } ], skyShadows: { framebuffer: \"my_shadow_framebuffer\", // These are up to your personal preference allowEntities: true, allowParticles: true, supportForwardRender: true, // You can name the shaders whatever you want, and place them // wherever you want, but make sure you specify the correct path // here, relative to your assets/namespace/... folder vertexSource: \"my_pipeline_namespace:shaders/.../shadow.vert\", fragmentSource: \"my_pipeline_namespace:shaders/.../shadow.frag\", // Parameters to glPolygonOffset(offsetSlopeFactor, offsetBiasUnits) offsetSlopeFactor: 1.1, offsetBiasUnits: 4.0, // Cascade radii are up to personal preference. // Aim for a balance between detail up close and detail far away. // // There are four cascades, notice that only three numbers are // supplied here. // This is because the 0th cascade always covers the largest // allowed shadow render distance. cascadeRadius: [96, 32, 12] }, sky: { // The angle of the sun. 0 means the sun follows the same // trajectory as vanilla. Negative numbers are allowed. defaultZenithAngle: 40 } } The fun shader part Now that we have the pipeline shadow descriptor done, we can move on to writing the actual shadow shaders. Both the vertex and fragment shaders are surprisingly simple, because you're not doing any fancy shading. The shadow program will be run multiple times, once for each cascade. The vertex shader Navigate to where you created your vertex shader. If you're wondering where a good place is, I like them in the same directory as the material program shaders, or the gbuffer shaders: assets/my_namespace/shaders/gbuffer/shadow.vert #include frex:shaders/api/vertex.glsl #include frex:shaders/api/view.glsl uniform int frxu_cascade; void frx_pipelineVertex() { // Move from model space to camera space frx_vertex += frx_modelToCamera; // Multiply for the shadow projection with the current cascade gl_Position = frx_shadowViewProjectionMatrix(frxu_cascade) * frx_vertex; } As you can see, this shader is very simple; all we need to do is transform vertices to the sun's point of view. The fragment shader Create the fragment shader in the path you decided upon earlier. #include frex:shaders/api/material.glsl #include frex:shaders/api/fragment.glsl void frx_pipelineFragment() { gl_FragDepth = gl_FragCoord.z; } This is even simpler than the vertex shader. Since there are no color attachments, all we need to do is write to depth. Now, if you reload your pipeline, you should be at a point where the shadow map is rendered properly! However, we want to use the shadow map to render shadows in our lighting system. Sampling shadows This section will be a little abstract, because shadow rendering can go from being very simple to very complicated, and there are a bunch of different moving parts. To be able to sample shadows, we need to first set up the shadow positions using some fancy transformations. Note: If you decide to do these transformations in the vertex shader, keep in mind that beyond 12-16 chunks render distance, there are likely to be more vertices than pixels on the screen, making your code slower. First, copy these helper functions into your code. They're annoying to write and give us the information we need regarding cascades, etc. vec3 shadowDist(int cascade, vec4 pos) { vec4 c = frx_shadowCenter(cascade); return abs((c.xyz - pos.xyz) / c.w); } // Function for obtaining the cascade level int selectShadowCascade(vec4 shadowViewSpacePos) { vec3 d3 = shadowDist(3, shadowViewSpacePos); vec3 d2 = shadowDist(2, shadowViewSpacePos); vec3 d1 = shadowDist(1, shadowViewSpacePos); if(all(lessThan(d3, vec3(1.0)))) { return 3; } if(all(lessThan(d2, vec3(1.0)))) { return 2; } if(all(lessThan(d1, vec3(1.0)))) { return 1; } return 0; } Now, let's set up the shadow position itself. In the code below, cameraSpacePos refers to the position in the same space as frx_vertex.xyz . If you are in the material program, you can use frx_vertex.xyz , otherwise you will need to set up this position yourself using matrix transformations. vec4 shadowViewPos = frx_shadowViewMatrix * vec4(cameraSpacePos, 1.0); int cascade = selectShadowCascade(shadowViewPos); vec4 shadowClipPos = frx_shadowProjectionMatrix(cascade) * shadowViewPos; vec3 shadowScreenPos = (shadowClipPos.xyz / shadowClipPos.w) * 0.5 + 0.5; Now that we have the shadow screen position, we can sample shadows. Another note: it's normal to be confused about the shadow transformations. They're pretty unintuitive. To keep it simple, shadowViewPos is the actual world-scaled position of the world from the sun's perspective, but with the origin and axes relative to the camera. shadowClipPos is the transformation of shadowViewPos into device coordinates so that geometry clipping can happen. shadowScreenPos is the screen-relative coordinates of the shadow map, and it's what we can use to sample the image. Think of it as texture coordinates, or texcoords for the shadow map. If you want to sample your shadow map in the material program: float shadowFactor = texture(frxs_shadowMap, vec4(shadowScreenPos.xy, cascade, shadowScreenPos.z)); float rawShadowDepth = texture(frxs_shadowMapTexture, vec3(shadowScreenPos.xy, cascade)).r; If you want to sample your shadow map in a pass shader or fullscreen pass, you will need to set up the sampler manually, using the name of the shadow map image you created. If you want to get the raw shadow factor, you should declare the sampler as sampler2DArrayShadow . If you want raw depth, declare the sampler as sampler2DArray . If you want both, set up two different samplers that point to the same image. // In the global scope uniform sampler2DArrayShadow u_myShadowMap; uniform sampler2DArray u_myShadowMapTexture; // In the place where you want factors float shadowFactor = texture(u_myShadowMap, vec4(shadowScreenPos.xy, cascade, shadowScreenPos.z)); float rawShadowDepth = texture(u_myShadowMapTexture, vec3(shadowScreenPos.xy, cascade)).r; Use the shadow factor you get from sampling the shadow map for whatever purpose you want! You can also sample the shadow map multiple times with tiny offsets to \"blur\" the shadows, causing less artifacts when moving. When you are comfortable with shadow rendering, you can look into more advanced shadow sampling techniques, like variable penumbra shadows, which adjusts the blur amount based on how far the shadow caster is to the ground. As a final note, you will notice that the back faces of blocks still let a bit of sunlight through. This is because simply applying the shadow factor to your lighting isn't exactly realistic - you should multiply by the dot product of the normal vector and the light vector. This multiplication both makes your lighting look more realistic and makes sure that the back faces of blocks are properly in shadow.","title":"Shadow map"},{"location":"pipeline/shadow_map/#the-shadow-map","text":"The shadow map is the image that stores the depth from the sun's perspective. By comparing the depth from the sun's point of view with the depth from the player's point of view, we can tell whether there are shadows. Since this requires rendering the entire scene a second time, it's significantly slower than not using shadows at all.","title":"The Shadow Map"},{"location":"pipeline/shadow_map/#the-boring-json-side","text":"To start, you need to create the shadow configuration JSON file. You can create this directly in your pipeline description file, or you can use another file and include the file in your pipeline description file. Let's assume you're doing the latter, and create my_shadow_config.json5 . This can be done anywhere under the assets folder. See comments for specific parameter descriptions. { images: [ { name: \"my_shadow_map\", // Size can be whatever you want, but it's recommended to use a power of 2, like 1024 size: 1024, internalFormat: \"DEPTH_COMPONENT32\", pixelFormat: \"DEPTH_COMPONENT\", pixelDataType: \"FLOAT\", target: \"TEXTURE_2D_ARRAY\", depth: 4, texParams: [ {name: \"TEXTURE_MIN_FILTER\", val: \"LINEAR\"}, {name: \"TEXTURE_MAG_FILTER\", val: \"LINEAR\"}, {name: \"TEXTURE_WRAP_S\", val: \"CLAMP_TO_EDGE\"}, {name: \"TEXTURE_WRAP_T\", val: \"CLAMP_TO_EDGE\"}, {name: \"TEXTURE_COMPARE_MODE\", val: \"COMPARE_REF_TO_TEXTURE\"}, {name: \"TEXTURE_COMPARE_FUNC\", val: \"LEQUAL\"} ] } ], framebuffers: [ { name: \"my_shadow_framebuffer\", // The shadow pass only writes to depth, so we provide a depth attachment depthAttachment: { image: \"my_shadow_map\", clearDepth: 1.0 }, } ], skyShadows: { framebuffer: \"my_shadow_framebuffer\", // These are up to your personal preference allowEntities: true, allowParticles: true, supportForwardRender: true, // You can name the shaders whatever you want, and place them // wherever you want, but make sure you specify the correct path // here, relative to your assets/namespace/... folder vertexSource: \"my_pipeline_namespace:shaders/.../shadow.vert\", fragmentSource: \"my_pipeline_namespace:shaders/.../shadow.frag\", // Parameters to glPolygonOffset(offsetSlopeFactor, offsetBiasUnits) offsetSlopeFactor: 1.1, offsetBiasUnits: 4.0, // Cascade radii are up to personal preference. // Aim for a balance between detail up close and detail far away. // // There are four cascades, notice that only three numbers are // supplied here. // This is because the 0th cascade always covers the largest // allowed shadow render distance. cascadeRadius: [96, 32, 12] }, sky: { // The angle of the sun. 0 means the sun follows the same // trajectory as vanilla. Negative numbers are allowed. defaultZenithAngle: 40 } }","title":"The boring JSON side"},{"location":"pipeline/shadow_map/#the-fun-shader-part","text":"Now that we have the pipeline shadow descriptor done, we can move on to writing the actual shadow shaders. Both the vertex and fragment shaders are surprisingly simple, because you're not doing any fancy shading. The shadow program will be run multiple times, once for each cascade.","title":"The fun shader part"},{"location":"pipeline/shadow_map/#the-vertex-shader","text":"Navigate to where you created your vertex shader. If you're wondering where a good place is, I like them in the same directory as the material program shaders, or the gbuffer shaders: assets/my_namespace/shaders/gbuffer/shadow.vert #include frex:shaders/api/vertex.glsl #include frex:shaders/api/view.glsl uniform int frxu_cascade; void frx_pipelineVertex() { // Move from model space to camera space frx_vertex += frx_modelToCamera; // Multiply for the shadow projection with the current cascade gl_Position = frx_shadowViewProjectionMatrix(frxu_cascade) * frx_vertex; } As you can see, this shader is very simple; all we need to do is transform vertices to the sun's point of view.","title":"The vertex shader"},{"location":"pipeline/shadow_map/#the-fragment-shader","text":"Create the fragment shader in the path you decided upon earlier. #include frex:shaders/api/material.glsl #include frex:shaders/api/fragment.glsl void frx_pipelineFragment() { gl_FragDepth = gl_FragCoord.z; } This is even simpler than the vertex shader. Since there are no color attachments, all we need to do is write to depth. Now, if you reload your pipeline, you should be at a point where the shadow map is rendered properly! However, we want to use the shadow map to render shadows in our lighting system.","title":"The fragment shader"},{"location":"pipeline/shadow_map/#sampling-shadows","text":"This section will be a little abstract, because shadow rendering can go from being very simple to very complicated, and there are a bunch of different moving parts. To be able to sample shadows, we need to first set up the shadow positions using some fancy transformations. Note: If you decide to do these transformations in the vertex shader, keep in mind that beyond 12-16 chunks render distance, there are likely to be more vertices than pixels on the screen, making your code slower. First, copy these helper functions into your code. They're annoying to write and give us the information we need regarding cascades, etc. vec3 shadowDist(int cascade, vec4 pos) { vec4 c = frx_shadowCenter(cascade); return abs((c.xyz - pos.xyz) / c.w); } // Function for obtaining the cascade level int selectShadowCascade(vec4 shadowViewSpacePos) { vec3 d3 = shadowDist(3, shadowViewSpacePos); vec3 d2 = shadowDist(2, shadowViewSpacePos); vec3 d1 = shadowDist(1, shadowViewSpacePos); if(all(lessThan(d3, vec3(1.0)))) { return 3; } if(all(lessThan(d2, vec3(1.0)))) { return 2; } if(all(lessThan(d1, vec3(1.0)))) { return 1; } return 0; } Now, let's set up the shadow position itself. In the code below, cameraSpacePos refers to the position in the same space as frx_vertex.xyz . If you are in the material program, you can use frx_vertex.xyz , otherwise you will need to set up this position yourself using matrix transformations. vec4 shadowViewPos = frx_shadowViewMatrix * vec4(cameraSpacePos, 1.0); int cascade = selectShadowCascade(shadowViewPos); vec4 shadowClipPos = frx_shadowProjectionMatrix(cascade) * shadowViewPos; vec3 shadowScreenPos = (shadowClipPos.xyz / shadowClipPos.w) * 0.5 + 0.5; Now that we have the shadow screen position, we can sample shadows. Another note: it's normal to be confused about the shadow transformations. They're pretty unintuitive. To keep it simple, shadowViewPos is the actual world-scaled position of the world from the sun's perspective, but with the origin and axes relative to the camera. shadowClipPos is the transformation of shadowViewPos into device coordinates so that geometry clipping can happen. shadowScreenPos is the screen-relative coordinates of the shadow map, and it's what we can use to sample the image. Think of it as texture coordinates, or texcoords for the shadow map. If you want to sample your shadow map in the material program: float shadowFactor = texture(frxs_shadowMap, vec4(shadowScreenPos.xy, cascade, shadowScreenPos.z)); float rawShadowDepth = texture(frxs_shadowMapTexture, vec3(shadowScreenPos.xy, cascade)).r; If you want to sample your shadow map in a pass shader or fullscreen pass, you will need to set up the sampler manually, using the name of the shadow map image you created. If you want to get the raw shadow factor, you should declare the sampler as sampler2DArrayShadow . If you want raw depth, declare the sampler as sampler2DArray . If you want both, set up two different samplers that point to the same image. // In the global scope uniform sampler2DArrayShadow u_myShadowMap; uniform sampler2DArray u_myShadowMapTexture; // In the place where you want factors float shadowFactor = texture(u_myShadowMap, vec4(shadowScreenPos.xy, cascade, shadowScreenPos.z)); float rawShadowDepth = texture(u_myShadowMapTexture, vec3(shadowScreenPos.xy, cascade)).r; Use the shadow factor you get from sampling the shadow map for whatever purpose you want! You can also sample the shadow map multiple times with tiny offsets to \"blur\" the shadows, causing less artifacts when moving. When you are comfortable with shadow rendering, you can look into more advanced shadow sampling techniques, like variable penumbra shadows, which adjusts the blur amount based on how far the shadow caster is to the ground. As a final note, you will notice that the back faces of blocks still let a bit of sunlight through. This is because simply applying the shadow factor to your lighting isn't exactly realistic - you should multiply by the dot product of the normal vector and the light vector. This multiplication both makes your lighting look more realistic and makes sure that the back faces of blocks are properly in shadow.","title":"Sampling shadows"},{"location":"pipeline/what_is_pipeline/","text":"What is a Pipeline? Canvas Pipeline is a configurable set of shader program and passes with the capability of shading the full game as well as GUI items. Additionally, a Pipeline also enables specific rendering features such as shadows, post-processing, and PBR (work in progress). What is a material shader? Game objects like terrain, entities, and particles are rendered with materials. Therefore the program used to render them is known as the Material Program. The Material Program is controlled by the Pipeline, but it may be extended by mods and resource packs via material shaders. To put it simply, material shaders control the early part of object rendering before it's being passed over to the Pipeline. There is also the depth-pass shader, which works like material shader but for shadow pass. For more details, see the Material Program and Material Shader sections. (TODO) What is a shader pack? In the past, custom Minecraft shaders are distributed as .zip files called \"shader pack\" (or simply \"shader\"). This holds true to this day. However, Canvas shaders are distributed as resource packs . These resource packs may contain: - One or more Pipeline(s), - Material shaders, - Material maps, - Language files, textures, and other common resources. As Canvas packs contain shaders, it makes sense to call them \"shader pack\" as well.","title":"What is a Pipeline?"},{"location":"pipeline/what_is_pipeline/#what-is-a-pipeline","text":"Canvas Pipeline is a configurable set of shader program and passes with the capability of shading the full game as well as GUI items. Additionally, a Pipeline also enables specific rendering features such as shadows, post-processing, and PBR (work in progress).","title":"What is a Pipeline?"},{"location":"pipeline/what_is_pipeline/#what-is-a-material-shader","text":"Game objects like terrain, entities, and particles are rendered with materials. Therefore the program used to render them is known as the Material Program. The Material Program is controlled by the Pipeline, but it may be extended by mods and resource packs via material shaders. To put it simply, material shaders control the early part of object rendering before it's being passed over to the Pipeline. There is also the depth-pass shader, which works like material shader but for shadow pass. For more details, see the Material Program and Material Shader sections. (TODO)","title":"What is a material shader?"},{"location":"pipeline/what_is_pipeline/#what-is-a-shader-pack","text":"In the past, custom Minecraft shaders are distributed as .zip files called \"shader pack\" (or simply \"shader\"). This holds true to this day. However, Canvas shaders are distributed as resource packs . These resource packs may contain: - One or more Pipeline(s), - Material shaders, - Material maps, - Language files, textures, and other common resources. As Canvas packs contain shaders, it makes sense to call them \"shader pack\" as well.","title":"What is a shader pack?"}]}